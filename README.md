# JobApp - Automated Job Board Aggregator

A comprehensive web application that automatically scrapes job listings from LinkedIn and provides a clean, searchable interface for job seekers. Built with FastAPI backend, React frontend, and Scrapy-based web scraping.

## ğŸš€ Features

- **Automated Scraping**: Continuously scrapes job listings from LinkedIn using Scrapy
- **Real-time Updates**: Live job feed with instant notifications and status tracking
- **Advanced Search**: Filter jobs by keywords, location, and other criteria
- **Clean UI**: Modern, responsive React interface with Tailwind CSS
- **RESTful API**: FastAPI backend with comprehensive endpoints
- **Database Storage**: MongoDB integration with job persistence
- **Logging System**: Detailed logging for debugging and monitoring
- **Task Management**: Background scraping tasks with status tracking

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - Modern, fast web framework for Python
- **MongoDB** - NoSQL database for job storage
- **Pydantic** - Data validation and settings management
- **Uvicorn** - ASGI server

### Frontend
- **React** - UI library
- **Vite** - Build tool
- **Axios** - HTTP client
- **Tailwind CSS** - Utility-first CSS framework

### Scraping
- **Scrapy** - Web scraping framework
- **Scrapyd** - Scrapy service daemon
- **BeautifulSoup** - HTML/XML parser

## ğŸ“ Project Structure

```
JobApp/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ settings.py     # Configuration settings
â”‚   â”‚   â”œâ”€â”€ db/             # Database models and connection
â”‚   â”‚   â”œâ”€â”€ routers/        # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py     # Job management endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.py    # Scraping task endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ resumes.py  # Resume management
â”‚   â”‚   â”‚   â””â”€â”€ email.py    # Email notifications
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic models
â”‚   â”‚   â””â”€â”€ services/       # Business logic
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ frontend/               # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx        # React entry point
â”‚   â”‚   â””â”€â”€ pages/
â”‚   â”‚       â””â”€â”€ App.jsx     # Main application component
â”‚   â”œâ”€â”€ package.json        # Node.js dependencies
â”‚   â””â”€â”€ index.html          # HTML template
â”œâ”€â”€ newscraper/             # Scrapy scraping service
â”‚   â”œâ”€â”€ jobscrapper/
â”‚   â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”‚   â””â”€â”€ linkedin_jobs.py  # LinkedIn job scraper
â”‚   â”‚   â””â”€â”€ settings.py     # Scrapy configuration
â”‚   â””â”€â”€ scrapy.cfg         # Scrapy project configuration
â”œâ”€â”€ scraper.zip             # Pre-configured scraper package
â””â”€â”€ test.py                 # Testing utilities
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Node.js 16+
- MongoDB
- Scrapy

### Backend Setup

1. Navigate to the backend directory:
```bash
cd backend
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Start the FastAPI server:
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Setup

1. Navigate to the frontend directory:
```bash
cd frontend
```

2. Install dependencies:
```bash
npm install
```

3. Start the development server:
```bash
npm run dev
```

### Scraping Service Setup

1. Navigate to the scraper directory:
```bash
cd newscraper
```

2. Install Scrapy:
```bash
pip install scrapy scrapyd
```

3. Deploy the spider:
```bash
scrapyd-deploy
```

## ğŸ“¡ API Endpoints

### Jobs
- `GET /jobs/` - Retrieve all jobs

### Tasks
- `POST /tasks/scrape` - Start new scraping task
- `GET /tasks/scrape/status/{task_id}` - Get scraping task status



## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the backend directory:

```env
MONGODB_URL=mongodb://localhost:27017
DATABASE_NAME=jobapp
SECRET_KEY=your-secret-key
```

### Scrapy Configuration

The LinkedIn scraper can be configured with:
- `domain`: Job role keywords (e.g., "software engineer")
- `location`: Job location (e.g., "San Francisco, CA")

## ğŸ§ª Usage

### Starting a Scraping Task

1. Open the frontend at `http://localhost:5173`
2. Enter job role keywords and location
3. Click "Scrape" to start the scraping task
4. Monitor the task status in real-time
5. View scraped jobs once complete

### Viewing Jobs

- Use the "Load Demo Jobs" button to load sample data
- Jobs are displayed with title, company, location, and direct link
- Click on job titles to open LinkedIn job pages

## ğŸ§ª Testing

Run the test suite:
```bash
python test.py
```

## ğŸ“ Development

### Adding New Job Sources

1. Create a new spider in `newscraper/jobscrapper/spiders/`
2. Update the scraping service in `backend/app/services/scrape.py`
3. Add new endpoints in `backend/app/routers/tasks.py`

### Frontend Customization

- Modify components in `frontend/src/pages/`
- Update styles in Tailwind CSS classes
- Add new features using React hooks

## ğŸ› Troubleshooting

### Common Issues

1. **CORS errors**: Ensure backend CORS middleware is configured correctly
2. **Database connection**: Check MongoDB is running and accessible
3. **Scraping failures**: LinkedIn may block requests - consider using rotating proxies
4. **Port conflicts**: Ensure ports 8000 (backend) and 5173 (frontend) are available

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- FastAPI team for the excellent framework
- Scrapy community for web scraping tools
- React team for the frontend library
- LinkedIn for job data (please respect their terms of service)